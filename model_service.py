# -*- coding: utf-8 -*-
# ì´ íŒŒì¼ì€ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ë¡œë”© ë° ì˜ˆì¸¡ ë¡œì§ì„ ì „ë‹´í•©ë‹ˆë‹¤.

import os
import joblib
import numpy as np
import pandas as pd
from typing import List, Tuple

# app.pyê°€ ì•„ë‹Œ ì—¬ê¸°ì„œ ì§ì ‘ schemasë¥¼ importí•©ë‹ˆë‹¤.
import schemas

# --- 6ê°œì˜ ëª¨ë¸/ì „ì²˜ë¦¬ê¸° íŒŒì¼ì„ ëª¨ë‘ ë¡œë“œ ---
base_dir = os.path.dirname(__file__)
model_dir = os.path.join(base_dir, "model")

MODEL_PATHS = {
    "mlp": os.path.join(model_dir, "mlp_model_6input.pkl"),
    "label_encoder": os.path.join(model_dir, "label_encoder_6input.pkl"),
    "scaler": os.path.join(model_dir, "scaler.pkl"),
    "zero_center": os.path.join(model_dir, "zero_center_means.pkl"),
    "soft_iron": os.path.join(model_dir, "soft_iron_matrix.pkl"),
    "hard_iron": os.path.join(model_dir, "bias.pkl")
}

models = {}
MODEL_LOADED = False
FEATURE_COLS = None
try:
    for name, path in MODEL_PATHS.items():
        models[name] = joblib.load(path)
        print(f"âœ… ëª¨ë¸ ì»´í¬ë„ŒíŠ¸ ë¡œë“œ ì™„ë£Œ: {os.path.basename(path)}")
    MODEL_LOADED = True
    FEATURE_COLS = ['Mag_X', 'Mag_Y', 'Mag_Z', 'Ori_X', 'Ori_Y', 'Ori_Z']
    print("ğŸš€ ëª¨ë“  ëª¨ë¸ ì»´í¬ë„ŒíŠ¸(ë³´ì • í¬í•¨)ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
except (FileNotFoundError, KeyError) as e:
    print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

# --- ì™¸ë¶€(app.py)ì—ì„œ ì‚¬ìš©í•  í•¨ìˆ˜ë“¤ ---

def get_model_status() -> dict:
    """ëª¨ë¸ì˜ ë¡œë“œ ìƒíƒœì™€ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return {
        "status": "ok" if MODEL_LOADED else "error",
        "model_loaded": MODEL_LOADED,
        "model_name": "MLPClassifier (with Full Calibration Pipeline)",
        "feature_cols": FEATURE_COLS if MODEL_LOADED else None,
    }

def run_prediction(data: schemas.SensorInput) -> Tuple[int, list, list]:
    """
    ì…ë ¥ ë°ì´í„°ë¥¼ ë°›ì•„ ë³´ì • ë° ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    (ê¸°ì¡´ app.pyì˜ run_full_prediction_pipeline í•¨ìˆ˜ì™€ ë™ì¼)
    """
    # 1. ì…ë ¥ ë°ì´í„°ë¥¼ Pandas DataFrameìœ¼ë¡œ ë³€í™˜
    feature_df = pd.DataFrame([data.model_dump(exclude={'top_k'})], columns=FEATURE_COLS)

    # 2. Hard-Iron ë³´ì •
    for col in ['Mag_X', 'Mag_Y', 'Mag_Z']:
        feature_df[col] -= models["hard_iron"][col]

    # 3. Soft-Iron ë³´ì •
    mag_data = feature_df[['Mag_X', 'Mag_Y', 'Mag_Z']].values
    feature_df[['Mag_X', 'Mag_Y', 'Mag_Z']] = np.dot(mag_data, models["soft_iron"].T)
    
    # 4. Zero-Centering
    for col in FEATURE_COLS:
        feature_df[col] -= models["zero_center"][col]
    
    # 5. ìŠ¤ì¼€ì¼ë§
    scaled_features = models["scaler"].transform(feature_df)
    
    # 6. MLP ì˜ˆì¸¡ ë° Label Decoding
    mlp_model = models["mlp"]
    prediction_index = mlp_model.predict(scaled_features)[0]
    final_prediction = int(models["label_encoder"].inverse_transform([prediction_index])[0])

    # 7. ì‹ ë¢°ë„ ë° Top-K ê³„ì‚°
    top_results_for_logging, top_results_for_response = [], []
    if hasattr(mlp_model, "predict_proba"):
        probabilities = mlp_model.predict_proba(scaled_features)[0]
        
        num_results_needed = max(3, data.top_k)
        top_indices = np.argsort(-probabilities)[:num_results_needed]
        
        top_labels = models["label_encoder"].inverse_transform(top_indices)
        top_probs = probabilities[top_indices]
        
        all_top_results = [(int(label), prob) for label, prob in zip(top_labels, top_probs)]

        top_results_for_logging = all_top_results[:3]
        top_results_for_response = all_top_results[:data.top_k]
        
    return final_prediction, top_results_for_logging, top_results_for_response

