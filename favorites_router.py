# -*- coding: utf-8 -*-
# ì´ íŒŒì¼ì€ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ë¡œë”© ë° ì˜ˆì¸¡ ë¡œì§ì„ ì „ë‹´í•©ë‹ˆë‹¤.

import os
import joblib
import numpy as np
import pandas as pd
from typing import Tuple
from collections import deque
import pywt  # ğŸ”§ ì›¨ì´ë¸Œë ›

# app.pyê°€ ì•„ë‹Œ ì—¬ê¸°ì„œ ì§ì ‘ schemasë¥¼ importí•©ë‹ˆë‹¤.
import schemas

# --- ëª¨ë¸/ì „ì²˜ë¦¬ê¸° íŒŒì¼ì„ ëª¨ë‘ ë¡œë“œ ---
base_dir = os.path.dirname(__file__)
model_dir = os.path.join(base_dir, "model")

# ğŸ”§ zero_center ì œê±° (ë” ì´ìƒ ì‚¬ìš©/ë¡œë“œí•˜ì§€ ì•ŠìŒ)
MODEL_PATHS = {
    "mlp": os.path.join(model_dir, "mlp_model_6input.pkl"),
    "scaler": os.path.join(model_dir, "scaler.pkl"),
    "label_encoder": os.path.join(model_dir, "label_encoder_6input.pkl"),
    "soft_iron": os.path.join(model_dir, "soft_iron_matrix.pkl"),
    "hard_iron": os.path.join(model_dir, "bias.pkl"),
}

models = {}
MODEL_LOADED = False
FEATURE_COLS = None

try:
    for name, path in MODEL_PATHS.items():
        models[name] = joblib.load(path)
        print(f"âœ… ëª¨ë¸ ì»´í¬ë„ŒíŠ¸ ë¡œë“œ ì™„ë£Œ: {os.path.basename(path)}")

    MODEL_LOADED = True
    FEATURE_COLS = ['Mag_X', 'Mag_Y', 'Mag_Z', 'Ori_X', 'Ori_Y', 'Ori_Z']
    print("ğŸš€ ëª¨ë“  ëª¨ë¸ ì»´í¬ë„ŒíŠ¸(6-input)ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
except (FileNotFoundError, KeyError) as e:
    print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

# ğŸ”§ ì›¨ì´ë¸Œë › ì„¤ì • & ë²„í¼
WAVELET_NAME = 'db4'
WAVELET_LEVEL = 3
WAVELET_MODE = 'soft'
BUFFER_SIZE = 64  # df ë‹¨ìœ„ê°€ ì•„ë‹ˆë¼ ìŠ¤íŠ¸ë¦¼ ë‹¨ìœ„ì—ì„œ ìµœê·¼ ìƒ˜í”Œì„ ëª¨ì•„ ì ìš©

# ìê¸°ì¥ 3ì¶•ì— ëŒ€í•œ ìˆœí™˜ ë²„í¼ (ì‹¤ì‹œê°„ ë‹¨ìƒ˜í”Œ ì…ë ¥ ëŒ€ì‘)
_mag_buffers = {
    'Mag_X': deque(maxlen=BUFFER_SIZE),
    'Mag_Y': deque(maxlen=BUFFER_SIZE),
    'Mag_Z': deque(maxlen=BUFFER_SIZE),
}

def _wavelet_denoise_1d(arr: np.ndarray,
                        wavelet: str = WAVELET_NAME,
                        level: int = WAVELET_LEVEL,
                        mode: str = WAVELET_MODE) -> np.ndarray:
    """
    Donoho universal threshold ê¸°ë°˜ 1D ì›¨ì´ë¸Œë › ë””ë…¸ì´ì¦ˆ.
    ì…ë ¥: 1D ndarray
    ì¶œë ¥: 1D ndarray (ë™ì¼ ê¸¸ì´)
    """
    if arr.size < 8:  # ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ìœ¼ë©´ ì›¨ì´ë¸Œë › ì˜ë¯¸ê°€ ì ìœ¼ë¯€ë¡œ ì›ì‹ í˜¸ ë°˜í™˜
        return arr

    coeffs = pywt.wavedec(arr, wavelet=wavelet, level=level)
    # ë…¸ì´ì¦ˆ í‘œì¤€í¸ì°¨ ì¶”ì • (ìµœìƒìœ„ detail ê³„ìˆ˜ì˜ MAD)
    if len(coeffs[-1]) == 0:
        return arr
    sigma = np.median(np.abs(coeffs[-1])) / 0.6745
    if sigma == 0.0:
        return arr

    uthresh = sigma * np.sqrt(2 * np.log(len(arr)))
    denoised_coeffs = [coeffs[0]] + [
        pywt.threshold(c, value=uthresh, mode=mode) for c in coeffs[1:]
    ]
    recon = pywt.waverec(denoised_coeffs, wavelet=wavelet)
    return recon[:len(arr)]

# --- ì™¸ë¶€(app.py)ì—ì„œ ì‚¬ìš©í•  í•¨ìˆ˜ë“¤ ---

def get_model_status() -> dict:
    """ëª¨ë¸ì˜ ë¡œë“œ ìƒíƒœì™€ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    return {
        "status": "ok" if MODEL_LOADED else "error",
        "model_loaded": MODEL_LOADED,
        "model_name": "MLP 6-input (Hard/Soft-Iron + Wavelet + Scaler)",  # ğŸ”§ ì„¤ëª… ì—…ë°ì´íŠ¸
        "feature_cols": FEATURE_COLS if MODEL_LOADED else None,
        "preprocess": {
            "hard_iron": True,
            "soft_iron": True,
            "wavelet": {"name": WAVELET_NAME, "level": WAVELET_LEVEL, "mode": WAVELET_MODE},
            "zero_centering": False,  # ğŸ”§ ì œê±°
            "scaler": True,
        }
    }

def extract_location_id(label_str):
    """ë¼ë²¨ì—ì„œ ìœ„ì¹˜ IDë§Œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜ (ê¸°ì¡´ê³¼ ë™ì¼)"""
    try:
        return int(str(label_str).split('_')[0])
    except (ValueError, IndexError):
        try:
            return int(label_str)
        except:
            return 0

def run_prediction(data: schemas.SensorInput) -> Tuple[int, list, list]:
    """
    6-input ëª¨ë¸ì— ë§ê²Œ ìˆ˜ë™ ì „ì²˜ë¦¬ ë° ì˜ˆì¸¡ ìˆ˜í–‰.
    ì „ì²˜ë¦¬ ìˆœì„œ: Hard-Iron â†’ Soft-Iron â†’ ğŸ”§ Wavelet â†’ Scaler â†’ MLP
    """
    if not MODEL_LOADED:
        raise RuntimeError("Model components are not loaded properly.")

    # 1) ì…ë ¥ â†’ DataFrame
    feature_df = pd.DataFrame([data.model_dump()], columns=FEATURE_COLS)

    # 2) Hard-Iron ë³´ì •
    for col in ['Mag_X', 'Mag_Y', 'Mag_Z']:
        feature_df[col] = feature_df[col].astype(float) - float(models["hard_iron"][col])

    # 3) Soft-Iron ë³´ì •
    mag_data = feature_df[['Mag_X', 'Mag_Y', 'Mag_Z']].values  # shape (1,3)
    feature_df[['Mag_X', 'Mag_Y', 'Mag_Z']] = np.dot(mag_data, models["soft_iron"].T)

    # 4) ğŸ”§ Wavelet Denoising (ì†Œí”„íŠ¸ì•„ì´ì–¸ ë’¤, ìŠ¤ì¼€ì¼ëŸ¬ ì „ì—)
    #    ì‹¤ì‹œê°„ ë‹¨ìƒ˜í”Œ ì˜ˆì¸¡ í˜¸í™˜ì„ ìœ„í•´, ì¶•ë³„ ë²„í¼ì— ëˆ„ì  â†’ ë²„í¼ ì „ì²´ë¥¼ ë””ë…¸ì´ì¦ˆ â†’ ë§ˆì§€ë§‰ ê°’ì„ ì‚¬ìš©
    for col in ['Mag_X', 'Mag_Y', 'Mag_Z']:
        _mag_buffers[col].append(float(feature_df[col].iloc[0]))
        buf_arr = np.asarray(_mag_buffers[col], dtype=float)
        denoised = _wavelet_denoise_1d(buf_arr)
        denoised_last = denoised[-1] if denoised.size > 0 else float(feature_df[col].iloc[0])
        feature_df.at[0, col] = denoised_last

    # ğŸ”§ (ì‚­ì œ) Zero-Centering ë‹¨ê³„ ì™„ì „ ì œê±°

    # 5) Scaler
    scaled_features = models["scaler"].transform(feature_df)

    # 6) MLP ì˜ˆì¸¡
    mlp_model = models["mlp"]
    prediction_index = mlp_model.predict(scaled_features)[0]

    # ë¼ë²¨ ë³µì› ë° ìœ„ì¹˜ ID ì¶”ì¶œ
    raw_prediction = models["label_encoder"].inverse_transform([prediction_index])[0]
    final_prediction = extract_location_id(raw_prediction)

    # 7) ì‹ ë¢°ë„ / Top-3 ìœ ë‹ˆí¬ í›„ë³´
    top_results_for_logging, top_results_for_response = [], []
    if hasattr(mlp_model, "predict_proba"):
        probabilities = mlp_model.predict_proba(scaled_features)[0]
        num_candidates = min(len(probabilities), 15)
        top_indices = np.argsort(-probabilities)[:num_candidates]
        raw_top_labels = models["label_encoder"].inverse_transform(top_indices)
        top_probs = probabilities[top_indices]

        seen_locations = set()
        unique_results = []
        for raw_label, prob in zip(raw_top_labels, top_probs):
            location_id = extract_location_id(raw_label)
            if location_id not in seen_locations:
                seen_locations.add(location_id)
                unique_results.append((location_id, float(prob)))
                if len(unique_results) >= 3:
                    break

        while len(unique_results) < 3:
            unique_results.append((0, 0.0))

        top_results_for_logging = unique_results[:3]
        top_results_for_response = unique_results[:3]

    return final_prediction, top_results_for_logging, top_results_for_response
